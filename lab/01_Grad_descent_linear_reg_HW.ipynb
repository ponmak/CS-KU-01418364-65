{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp7NtbehH-b2"
   },
   "source": [
    "## ML Review and Gradient Descent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hKtw9sYBH-b4"
   },
   "source": [
    "In this notebook, we will solve a simple linear regression problem by gradient descent.  \n",
    "We will see the effect of the learning rate on the trajectory in parameter space.\n",
    "We will show how Stochastic Gradient Descent (SGD) differs from the standard version, and the effect of \"shuffling\" your data during SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:33:18.172928Z",
     "start_time": "2023-07-09T15:33:18.166929Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1732665519881,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "-cUNz8L0H-b5",
    "outputId": "226a998d-f5cd-421c-b20e-56b21b1841d8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:33:20.284818Z",
     "start_time": "2023-07-09T15:33:19.667672Z"
    },
    "executionInfo": {
     "elapsed": 1534,
     "status": "ok",
     "timestamp": 1732665521407,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "fDqgB0z1H-b6"
   },
   "outputs": [],
   "source": [
    "# Preliminaries - packages to load\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOVumTv6H-b6"
   },
   "source": [
    "## Generate Data from a known distribution\n",
    "Below we will generate data a known distribution.  \n",
    "Specifically, the true model is:\n",
    "\n",
    "$Y = b + \\theta_1 X_1 + \\theta_2 X_2 + \\epsilon$\n",
    "\n",
    "$X_1$ and $X_2$ have a uniform distribution on the interval $[0,10]$, while `const` is a vector of ones (representing the intercept term).\n",
    "\n",
    "We set actual values for $b$ ,$\\theta_1$, and $\\theta_2$\n",
    "\n",
    "Here $b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n",
    "\n",
    "We then generate a vector of $y$-values according to the model and put the predictors together in a \"feature matrix\" `x_mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:33:23.634200Z",
     "start_time": "2023-07-09T15:33:23.626688Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732665521407,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "g91hm7S0Kcuj",
    "outputId": "972f47b8-0763-4828-e6e2-430ad1f9fce9"
   },
   "outputs": [],
   "source": [
    "np.random.uniform(0, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:33:24.850052Z",
     "start_time": "2023-07-09T15:33:24.841544Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732665521407,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "YfYhN3oWH-b7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)  ## This ensures we get the same data if all of the other parameters remain fixed\n",
    "\n",
    "num_obs = 100\n",
    "x1 = np.random.uniform(0, 10, num_obs)\n",
    "x2 = np.random.uniform(0, 10, num_obs)\n",
    "const = np.ones(num_obs)\n",
    "eps = np.random.normal(0, 0.5, num_obs)\n",
    "\n",
    "b = 1.5\n",
    "theta_1 = 2\n",
    "theta_2 = 5\n",
    "\n",
    "y = b*const + theta_1*x1 + theta_2*x2 + eps\n",
    "\n",
    "x_mat = np.array([const, x1, x2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTpTYQVGH-b7"
   },
   "source": [
    "## Get the \"Right\" answer directly\n",
    "In the below cells we solve for the optimal set of coefficients.  Note that even though the true model is given by:\n",
    "\n",
    "$b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n",
    "\n",
    "The maximum likelihood (least-squares) estimate from a finite data set may be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:45.154442Z",
     "start_time": "2023-07-09T15:09:39.517225Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3571,
     "status": "ok",
     "timestamp": 1732665524973,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "CNTuUag2H-b8",
    "outputId": "8169e69a-abee-4272-9a09-bd136eb26c76"
   },
   "outputs": [],
   "source": [
    "### Solve directly using sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(fit_intercept=False)\n",
    "lr_model.fit(x_mat, y)\n",
    "\n",
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "OfbG8hk0H-b8"
   },
   "source": [
    "## Exercise:\n",
    "Solve the problem two ways:\n",
    "1. By using the scikit-learn LinearRegression model\n",
    "2. Using matrix algebra directly via the formula $\\theta = (X^T X)^{-1}X^Ty$\n",
    "\n",
    "Note: The scikit-learn solver may give a warning message, this can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:45.200786Z",
     "start_time": "2023-07-09T15:09:45.158472Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1732665524973,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "hNQWI1OGH-b9",
    "outputId": "82186995-d43c-4fe4-b4d9-49a1d0854e49"
   },
   "outputs": [],
   "source": [
    "## Solve by matrix calculation\n",
    "np.linalg.inv(np.dot(x_mat.T, x_mat)).dot(x_mat.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB9BoIl_H-b9"
   },
   "source": [
    "# Solving by Gradient Descent\n",
    "Another way to solve this problem is to use the method of Gradient Descent.  We will explore this method because (as we will see) Neural Networks are trained by Gradient Descent.  Seeing how gradient descent works on a simple example will build intuition and help us understand some of the nuances around setting the learning rate.  We will also explore Stochastic Gradient Descent and compare its behavior to the standard approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq-bIh2bH-b9"
   },
   "source": [
    "## Exercise (Gradient Descent)\n",
    "\n",
    "The next several cells have code to perform (full-batch) gradient descent.  We have omitted some parameters for you to fill in.\n",
    "\n",
    "1. Pick a learning rate, and a number of iterations, run the code, and then plot the trajectory of your gradient descent.\n",
    "1. Find examples where the learning rate is too high, too low, and \"just right\".\n",
    "1. Look at plots of loss function under these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:45.215982Z",
     "start_time": "2023-07-09T15:09:45.204923Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1732665524973,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "KxRu1Gr3H-b-"
   },
   "outputs": [],
   "source": [
    "## Parameters to play with\n",
    "learning_rate = 0.1\n",
    "num_iter = 1000\n",
    "theta_initial = np.array([3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:45.280076Z",
     "start_time": "2023-07-09T15:09:45.219934Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1732665524974,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "HVHd9XpLH-b-",
    "outputId": "af465844-16eb-4f28-f438-af0e8b914783"
   },
   "outputs": [],
   "source": [
    "## Initialization steps\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros((num_iter+1, 3))\n",
    "theta_path[0, :] = theta_initial\n",
    "\n",
    "loss_vec = np.zeros(num_iter)\n",
    "\n",
    "## Main Gradient Descent loop (for a fixed number of iterations)\n",
    "for i in range(num_iter):\n",
    "    y_pred = np.dot(theta.T, x_mat.T)\n",
    "    loss_vec[i] = np.sum((y_pred - y)**2)\n",
    "    grad_vec = (y_pred - y).dot(x_mat)/num_obs  #sum up the gradients across all observations and divide by num_obs\n",
    "    theta = theta - learning_rate*grad_vec\n",
    "    theta_path[i+1, :] = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:47.396024Z",
     "start_time": "2023-07-09T15:09:45.288355Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "executionInfo": {
     "elapsed": 3915,
     "status": "ok",
     "timestamp": 1732665528875,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "yddh1KrxH-b-",
    "outputId": "3e09d74f-1573-498f-bf28-1ad1dbf606f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the results - it is a 3d parameter space - we plot 2d slices\n",
    "## Green is starting point and blue is ending point\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.rcParams['xtick.labelsize'] = plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['legend.fontsize'] = 20\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(theta_path[:, 1], theta_path[:, 2], 'k-x')\n",
    "plt.plot(theta_path[0, 1], theta_path[0, 2], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 1], theta_path[-1, 2], 'bo', markersize=15)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(theta_path[:, 0], theta_path[:, 1], 'k-x')\n",
    "plt.plot(theta_path[0, 0], theta_path[0, 1], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 0], theta_path[-1, 1], 'bo', markersize=15)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:, 0], theta_path[:, 2], 'k-x')\n",
    "plt.plot(theta_path[0, 0], theta_path[0, 2], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 0], theta_path[-1, 2], 'bo', markersize=15)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0, 500])\n",
    "\n",
    "## Plot the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXToIXCCH-b_"
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "Rather than average the gradients across the whole dataset before taking a step, we will now take a step for every datapoint.  Each step will be somewhat of an \"overreaction\" but they should average out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU0EV6Z0H-b_"
   },
   "source": [
    "## Exercise (Stochastic Gradient Descent)\n",
    "The below code runs Stochastic Gradient descent, but runs through the data in the same order every time.  \n",
    "\n",
    "1. Run the code and plot the graphs.  What do you notice?\n",
    "1. Modify the code so that it randomly re-orders the data.  How do the sample trajectories compare? [STUDENT TO COMPLETE THIS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:47.410828Z",
     "start_time": "2023-07-09T15:09:47.400634Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1732665528875,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "lO7ul1jUH-b_"
   },
   "outputs": [],
   "source": [
    "## Parameters to play with\n",
    "learning_rate = 0.1\n",
    "num_iter = 10   # The number of \"steps\" will be num_iter * numobs\n",
    "theta_initial = np.array([3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:47.442013Z",
     "start_time": "2023-07-09T15:09:47.418836Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1732665528875,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "90ozaBH-H-b_"
   },
   "outputs": [],
   "source": [
    "## Initialization steps\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros(((num_iter*num_obs)+1, 3))\n",
    "theta_path[0, :] = theta_initial\n",
    "loss_vec = np.zeros(num_iter*num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:47.520848Z",
     "start_time": "2023-07-09T15:09:47.447021Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1732665528875,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "vJu4Ot4LH-b_",
    "outputId": "bf77e668-f422-427e-b1d7-1f6b83250b16"
   },
   "outputs": [],
   "source": [
    "## Main SGD loop\n",
    "count = 0\n",
    "for i in range(num_iter):\n",
    "    for j in range(num_obs):\n",
    "        count += 1\n",
    "        y_pred = np.dot(theta.T, x_mat.T)\n",
    "        loss_vec[count-1] = np.sum((y_pred - y)**2)\n",
    "        grad_vec = (y_pred[j] - y[j])*(x_mat[j,:])\n",
    "        theta = theta - learning_rate*grad_vec\n",
    "        theta_path[count, :] = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:49.291434Z",
     "start_time": "2023-07-09T15:09:47.525852Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "executionInfo": {
     "elapsed": 2163,
     "status": "ok",
     "timestamp": 1732665531029,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "vHMJDiNxH-cA",
    "outputId": "dc1981d5-73dd-4eb6-bfb1-966f79ec79e1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot the results - it is a 3d parameter space - we plot 2d slices\n",
    "## Green is starting point and blue is ending point\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(theta_path[:, 1], theta_path[:, 2], 'k-x')\n",
    "plt.plot(theta_path[0, 1], theta_path[0, 2], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 1], theta_path[-1, 2], 'bo', markersize=15)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(theta_path[:, 0], theta_path[:, 1], 'k-x')\n",
    "plt.plot(theta_path[0, 0], theta_path[0, 1], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 0], theta_path[-1, 1], 'bo', markersize=15)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:, 0], theta_path[:, 2], 'k-x')\n",
    "plt.plot(theta_path[0, 0], theta_path[0, 2], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 0], theta_path[-1, 2], 'bo', markersize=15)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T15:09:49.307473Z",
     "start_time": "2023-07-09T15:09:49.296353Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1732665531030,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "w1D7M3QWH-cA"
   },
   "outputs": [],
   "source": [
    "## Student to write code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjcAm3V0VH-K"
   },
   "source": [
    "## Exercise (Mini Batch Gradient Descent)\n",
    "The below code runs Mini Batch Gradient Descent.  \n",
    "\n",
    "1. Pick a batch size, run the code, and then plot the trajectory of your gradient descent.\n",
    "1. Observe and explain how the batch size affects the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1732665531030,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "4FeXzFTZVWW3",
    "outputId": "0c7fd95a-6591-4e3f-9eb6-f2d78a682841"
   },
   "outputs": [],
   "source": [
    "## Parameters to play with\n",
    "learning_rate = 0.1\n",
    "num_iter = 1000\n",
    "batch_size = 10\n",
    "steps_per_epoch = num_obs // batch_size + int(num_obs % batch_size != 0)\n",
    "num_epoch = num_iter//steps_per_epoch  # The number of \"steps\" will be num_iter * num_obs / batch_size\n",
    "theta_initial = np.array([3, 3, 3])\n",
    "\n",
    "## Initialization steps\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros(((num_epoch*steps_per_epoch)+1, 3))\n",
    "theta_path[0, :]= theta_initial\n",
    "loss_vec = np.zeros(num_epoch*steps_per_epoch)\n",
    "\n",
    "count = 0\n",
    "idx = np.arange(num_obs)\n",
    "for i in range(num_epoch):\n",
    "    np.random.shuffle(idx)\n",
    "    y     = y[idx]\n",
    "    x_mat = x_mat[idx]\n",
    "    for j in range(steps_per_epoch):\n",
    "        count += 1\n",
    "        beg = j * batch_size\n",
    "        end = (j + 1) * batch_size\n",
    "        y_pred = np.dot(theta.T, x_mat.T)\n",
    "        loss_vec[count-1] = np.sum((y_pred - y)**2)\n",
    "        grad_vec = (y_pred[beg:end] - y[beg:end]).dot(x_mat[beg:end, :])/batch_size\n",
    "        theta = theta - learning_rate*grad_vec\n",
    "        theta_path[count, :] = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "executionInfo": {
     "elapsed": 1953,
     "status": "ok",
     "timestamp": 1732665532974,
     "user": {
      "displayName": "Chakrit WATCHAROPAS",
      "userId": "10640034624063521652"
     },
     "user_tz": -420
    },
    "id": "kO1nGyGJW1A7",
    "outputId": "d79e7057-440c-4069-ac33-e59775835b6f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot the results - it is a 3d parameter space - we plot 2d slices\n",
    "## Green is starting point and blue is ending point\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(theta_path[:, 1], theta_path[:, 2], 'k-x')\n",
    "plt.plot(theta_path[0, 1], theta_path[0, 2], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 1], theta_path[-1, 2], 'bo', markersize=15)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(theta_path[:, 0], theta_path[:, 1], 'k-x')\n",
    "plt.plot(theta_path[0, 0], theta_path[0, 1], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 0], theta_path[-1, 1], 'bo', markersize=15)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:, 0], theta_path[:, 2], 'k-x')\n",
    "plt.plot(theta_path[0, 0], theta_path[0, 2], 'yo', markersize=15)\n",
    "plt.plot(theta_path[-1, 0], theta_path[-1, 2], 'bo', markersize=15)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C73N-avbZ4V"
   },
   "source": [
    "# ขอให้ปรับแก้ไขโค้ดตามลักษณะของคำถาม แล้วจึงตอบคำถามต่อไปนี้\n",
    "### โดยให้เพิ่ม Cell สำหรับแต่ละคำถาม เพื่อใช้เขียนโค้ดและใช้ผลประกอบคำอธิบาย\n",
    "\n",
    "1. (Gradient Descent) หากค่า learning rate สูงหรือต่ำจนเกินไป มีผลต่อการลู่ของค่าพารามิเตอร์เข้าหาค่าที่ควรจะเป็นอย่างไร\n",
    "1. (Stochastic Gradient Descent) การสลับลำดับของข้อมูลที่ใช้ในการหาค่า gradient ส่งผลต่อกราฟ loss อย่างไร\n",
    "1. (Mini Batch Gradient Descent) ค่าของ batch size มีผลต่อการเทรนโมเดลอย่างไร\n",
    "1. (Mini Batch Gradient Descent) จำนวน epoch ที่สูงหรือต่ำจนเกินไป มีผลต่อการเทรนโมเดลอย่างไร"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
